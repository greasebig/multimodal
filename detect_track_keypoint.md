
# 检测

## 检测原理
为什么是小尺寸特征图用于检测大尺寸物体？      
感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域，如图1所示。    
![alt text](assets_picture/detect_track_keypoint/image-39.png)    
图中是个微型CNN，来自Inception-v3论文，原图是为了说明一个conv5x5可以用两个conv3x3代替，从下到上称为第1, 2, 3层：    
简而言之：某一层feature map(特性图)中某个位置的特征向量，是由前面某一层固定区域的输入计算出来的，那这个区域就是这个位置的感受野。任意两个层之间都有位置—感受野对应关系，但我们更常用的是feature map层到输入图像的感受野，如目标检测中我们需要知道feature map层每个位置的特征向量对应输入图像哪个区域，以便我们在这个区域中设置anchor，检测该区域内的目标。    

感受野作用

一般task要求感受野越大越好，如图像分类中最后卷积层的感受野要大于输入图像，网络深度越深感受野越大性能越好      
密集预测task要求输出像素的感受野足够的大，确保做出决策时没有忽略重要信息，一般也是越深越好     
目标检测task中设置anchor要严格对应感受野，anchor太大或偏离感受野都会严重影响检测性能   



目标检测是分类与定位的结合    
目标检测目前有两类流行算法:  
- 一类是基于Region Proposal的R-cnn系列（比如r-cnn,fast r-cnn,faster r-cnn）,他们是属于two stage的，   
需要先使用Selective search 或者cnn网络（RPN）来产生Region proposal,然后再对Region proposal上做分类和回归。    
所谓回归：指的是回归出四个坐标点的位置     
回归是统计学中的一种强大的算法，用于研究一组随机变量(Y1, Y2, ..., Yi)和另一组变量(X1, X2, ..., Xk)之间的关系   

- 另一类就是yolo，ssd系列的one stage算法，它仅仅使用一个cnn来直接预测不同目标的类别和位置

### yolo v1 算法原理
two-stage算法将步骤一与步骤二分开执行，输入图像先经过候选框生成网络（例如faster rcnn中的RPN网络），再经过分类网络    
yolo是通过一个cnn网络模型来实现end-to-end的目标检测    
Yolo很快，将detection视为回归问题，仅使用一个neural network同时预测bounding box的位置和类别，因此速度很快     
Yolo由于不需提取region proposal(区域提案)，而是直接在整幅图像进行检测，因此YOLOv1可以联系上下文信息和特征，减少将背景检测为物体的错误    
![alt text](assets_picture/yolo/image-4.png)   
非极大值抑制的方法进行筛选   
nms是怎么做筛选，如何计算分数筛选的？    

![alt text](assets_picture/detect_track_keypoint/image.png)    
在YOLO1中，YOLO的输出是一个SS(B5 + C)的张量   

S*S 模型最初将整个图像数据切分为S * S的网格图像，也就是长和宽都分别分成S分，从而获得SS个网格子图像。**那模型是如何控制输出SS格的预测的呢，是通过设置最后输出张量的第1,2数轴为S实现的。**    
B 模型的一个超参数，通过设置每个网格需要输出多少个边界框   
5 模型每个边界框有5个参数，（x , y , w, h, confs)   
C 模型需要学习的分类类别，因为机器学习里都是将分类类别独热化处理（one-hot)，因此模型需要全局学习多少个类别，C就是多少。需要注意的是，论文中指出每个网格只预测一组类别，无论对于每个网格需要预测多少个边界框B,也就是默认该网格中的输出B个边界框都是相同的类别。 每个网格单元只负责检测一个物体，因此只需要为每个网格单元预测一个class probabilities（类别概率），而不是为每个边界框都预测。 这样可以避免模型过度接近于较容易检测到的对象？？？？，而忽略了更难以检测到的对象。同时，这也减少了前向传播过程中的计算量，从而提高了模型的效率。    
在YOLO1中，作者给出了YOLO1的模型输出具体参数
```
S = 7
B = 2
C = 20
输出 为 7 * 7 * 30 的张量
```

B个边界框如何获取？一个网格的。    
分类获取好理解，输出交叉熵。回归是怎么回归的？将“10种分类”扩大化？坐标轴所有值都是分类对象。     

(x, y) 坐标位置代表边界框相对于网格边界的位置   
(w, h) 检测目标的长宽是相对于整个图像的长宽，即归一化后的    
confidence 置信度是预测边界框和实际中的框体之间的IOU    

#### 置信度 iou
根据这些计算结果，可以通过二分类模型（即物体是否存在的判断）和回归模型（即边界框的位置和尺寸）来预测每个边界框的置信度。    
在YOLO算法中，置信度被定义为包含物体的概率与IOU得分的乘积，即objectness score = Pr(object) × IOU。其中，Pr(object) 表示一个给定边界框所包含的物体的概率，它是通过对数据集进行标注来得到的，而 IOU 分数则是通过预测的边界框与真实边界框之间的IOU值计算得出的。 对于每个边界框，如果其 IOU 值大于一定阈值 (一般使用0.5)，则认为此边界框与其对应的物体高度匹配，否则，该边界框被视为负样本（即不包含物体）。至此，我们就可以在训练过程中计算出每个边界框的置信度，从而可以对识别结果进行排序和筛选，保留置信度最高的预测结果作为最终的检测结果。     

对于每个矩形框而言，我们需要在数据集中检查该矩形框所属的物体类别是否与当前训练的模型所预测的类别匹配。如果不匹配，则该矩形框被视为背景，属于负样本(不包含目标）；如果匹配，则该矩形框被视为正样本（包含目标）。    
对于每个正样本，我们需要计算该矩形框所包含物体的概率，即 Pr(object) 值。在YOLO算法中，Pr(object) 由两部分组成：一部分是矩形框中心所属的网格单元格所负责的边界框数目的倒数，另一部分则是目标物体中心点与矩形框中心点之间的距离（按单元格大小归一化）。具体地，对于每个正样本矩形框而言，其 Pr(object) 值由如下公式计算：    
![alt text](assets_picture/detect_track_keypoint/QQ图片20240308161431.png)
α是一个超参数，通常为1，IOU则是预测边界框和真实边界框之间的IOU得分。这些正样本矩形框的 Pr(object) 值都是通过标注信息来计算得到，在训练过程中反向传播梯度进行训练，从而让模型能够自适应地学习到每个矩形框是否包含物体这一信息。      
其中，第一项1/S^2 表示网络预测的该边界框是否包含目标的先验概率，也就是说，如果没有目标，在采样一张图片中预测出包含目标的边界框的概率也是很小的，因此引入一个先验概率。第二项α × IOU为IoU的置信度值，它用来表示边界框是否与目标框相似。其中，α是一个超参数，用来平衡先验概率和IoU的贡献。    

在YOLOv2及之后的版本中，目标置信度的计算方式被改进为只考虑交并比的影响。具体而言，目标置信度的计算公式为Pr(object) =IOU，其中，IOU为预测框与真实框之间的交并比。如果IOU大于某个设定的阈值，则被认为这个预测框框住的区域内存在一个物体。如果IOU 小于阈值，则将其置信度设为0，表示该预测框不包含目标。    

由于每一个单元格有多个边界框，但是每一个单元格其对应类别只有一个，如果在训练时，多个边界框存在目标，那就只选择与真实边框（ground truth）的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的结果使每一个单元格只对应一个边框，一个类别。   
大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。     



#### 一般性问题，细节
交叉熵损失函数    
交叉熵是用来衡量两个概率分布的距离。    
![alt text](assets_picture/detect_track_keypoint/image-11.png)



在YOLO中，使用相对于网格的坐标位置作为输出的x,y坐标可以帮助提升检测的精度。    
那么YOLO中的输出x,y为什么是相对于网格的相对坐标位置，而不直接用预测相对于整张图像的相对坐标位置
将bounding box的位置表示为相对于网格的坐标位置，相对于图像绝对位置的变化对网络输出的影响就会更小。这意味着模型会更加关注局部特征，而不会受到图像中不同目标之间的绝对位置关系的影响，从而可以更好地进行物体检测。   
此外，使用相对于网格的坐标位置也有助于缩小目标的坐标范围，从而使坐标值的范围集中在0到1之间，避免了模

问题1/2   
位置预测误差和置信度误差是等权重的，这显然是不合理的。8维(B * 4) 的位置信息 与20维©的类别信息的重视程度是不一样的    
在每张图片中，许多网格单元格都不包含任何物体。这会将这些单元格的“置信度”得分推向零，常常压制那些确实包含物体的单元格的梯度。也就是说这些零的置信度会在方向传播的过程中影响权重的更新，因为没有物体的网格的目标值是0。一方面，不包含任何物体的网格单元数量多，从而其对误差的影响大。另一方面，置信度 confidence 等于0， 可能会使模型存在梯度消失的问题。 进而压制包含物体的单元格的权重更新。    
这里为什么不对不包含任何物体的网格的坐标信息也加以限制呢，他们应该也是0之类的值？？？？？因为对于包含物体的网格，模型采用回归分支输出的bounding box坐标信息、分类分支输出的类别置信度信息以及该bounding box内物体出现的置信度信息共同构成损失函数，而对于不包含物体的网格，只采用该bounding box的置信度信息计算损失函数。该怎么理解呢，当我们知道一个框体的置信度为0之后，我们就不会去关注他的坐标信息了？？？？？？。所以在损失函数中，对于不包含具体物体的框体，我们只计算其置信度误差 confidence error。     
对于问题1和问题2，论文中通过对不同误差来源进行加权处理，从而使更重要的部分的误差对模型的影响更大。增加了边界框坐标预测的损失并减少了不包含物体的框的置信度预测的损失。 使用两个参数，λcoord和λnoobj来完成这个过程。设置λcoord = 5和λnoobj= .5。如果将所有的损失函数都设置为相同的权重，模型很容易受到这些值为0的bounding box的损失函数的影响，从而无法有效地往其他方向调整，导致训练不稳定以及收敛困难。因此，YOLO1中采用的这个不对称的损失函数来平衡这个问题。在这种方式下，当不包含物体的网格的预测置信度为0时，该网格的损失函数权重为λnoobj，远远小于包含物体的网格的损失函数权重λcoord。这种不对称的权重设置有效地平衡了这个问题，提高了训练的稳定性。    

要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。    
![alt text](assets_picture/detect_track_keypoint/image-9.png)
第一行的式子表示边界框中心坐标的误差，第二行式子表示边界框的宽高误差，第三行式子表示含有目标的边界框的置信度误差，第四项式子表示不含有目标的边界框的置信度误差，第五行式子表示含有目标的单元格的类别误差      
这里注意置信度Ci的值，如果不存在目标，则Pr(object)=0,那么置信度Ci=0,如果存在目标，则Pr(object)=1，需要确定值，才能得到置信度Ci的值；为了方便计算，你可以将Ci置为1；   

问题3   
由于图片中的边界框（Boundary box) 往往大小不一。对于大框来说，比如10单位的大框，0.5的位置偏差是能够接受的（1/20)；但是对于小框来说，比如2单位的小框，0.5的位置偏差就会代买明显的视觉偏差了(5/20)。也就是说大方框中的小偏差比小方框中的小偏差更不重要。 为了部分地解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。     
对于问题3，论文中，通过将box的width和height取平方根代替原本的height和width。 如下图：small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。    
![alt text](assets_picture/detect_track_keypoint/image-1.png)   

#### 具体损失函数    
YOLO算法是将目标检测看出一个回归问题，所以将均方差作为损失函数（所以不是分类问题不是交叉熵）     
损失函数分为定位误差部分和分类误差部分，对于不同部分他们的比重值λ    


参数：   
- λcoord    
更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为在pascal VOC训练中取5。    
- λnoobj    
对没有object的box的confidence loss，赋予小的loss weight，记为在pascal VOC训练中取0.5。
类别权重等于1       
- 有object的box的confidence loss和类别的loss的loss weight正常取1。    

![alt text](assets_picture/detect_track_keypoint/image-2.png)    
![alt text](assets_picture/detect_track_keypoint/image-3.png)   
对于包含物体的网格，模型采用回归分支输出的bounding box坐标信息、分类分支输出的类别置信度信息以及该bounding box内物体出现的置信度信息共同构成损失函数，而对于不包含物体的网格，只采用该bounding box的置信度信息计算损失函数。     
第一行的式子表示边界框中心坐标的误差，第二行式子表示边界框的宽高误差，第三行式子表示含有目标的边界框的置信度误差，第四项式子表示不含有目标的边界框的置信度误差，第五行式子表示含有目标的单元格的类别误差      
这里注意置信度Ci的值，如果不存在目标，则Pr(object)=0,那么置信度Ci=0,如果存在目标，则Pr(object)=1，需要确定值，才能得到置信度Ci的值；为了方便计算，你可以将Ci置为1；    


其中    
边界框定位误差    
![alt text](assets_picture/detect_track_keypoint/1709888864556.png)
![alt text](assets_picture/detect_track_keypoint/1709888920621.png)
![alt text](assets_picture/detect_track_keypoint/1709888944244.png)

边界框尺寸误差    
![alt text](assets_picture/detect_track_keypoint/1709888995993.png)    

置信度误差      
![alt text](assets_picture/detect_track_keypoint/1709890248948.png)   

分类误差   
![alt text](assets_picture/detect_track_keypoint/1709890433727.png)


在YOLO算法中，如果一个目标的中心点位于某个网格内部，那么算法会在该网格内部拟合一个边界框。边界框是一个矩形，其包含了目标的位置和大小信息。YOLO算法会同时预测每个网格内可能的边界框，因此可能会存在重叠的边界框。在后续处理中，可以使用非极大值抑制（NMS）算法去掉重复的边界框，只保留最可能的那一个，从而得到最终的目标边界框。     
当一个物体的边界框跨越了多个网格时，每个包含该边界框中心点的网格都会尝试预测该边界框。在这种情况下，YOLO并不会将该物体视为单个网格上的物体。而是采用一种特殊的处理方式，将该物体的边界框分配给两个网格中置信度最高的那个。这种处理方式称为“跨越边界框（Cross-boundary box）”，它允许处理跨越网格的物体以有效地增加算法的检测能力。     

非极大值抑制（Non-Maximum Suppression，简称NMS）是一种常见的目标检测后处理方法，用于移除多余的边界框。NMS算法的原理是，对于每个类别的边界框，按照其置信度（或得分）从高到低进行排序。然后，选择置信度最高的边界框，将其它与之高度重叠的边界框（即IOU（Intersection over Union）重叠率大于某个阈值）从列表中移除。重复这个步骤，选择下一个最高置信度的边界框，直到所有的边界框都被处理完毕。    
在YOLO算法中，NMS算法通常是用于筛选检测到的目标边界框，以保留每个目标的最终检测结果。NMS算法可以剔除低于置信度阈值的边界框，同时去除重复的边界框，从而使目标检测结果更准确。通常情况下，NMS算法使用的重叠率阈值为0.5，可以根据具体任务和实验来调整。        
“重复这个步骤”指的是从剩余的未处理边界框列表中选择置信度第二高的边界框，然后再次进行非极大值抑制过程，直到所有边界框都被处理完成。    
重叠的边界框对于NMS算法来说可能会被移除，而最终保留的边界框是非极大值抑制算法的输出结果。例如，如果两个边界框的重叠率（Intersection over Union，IOU）大于设定的阈值，则只会保留置信度更高的那个边界框，而将置信度较低的边界框进行删除。    

NMS算法计算的是同一类别的边界框之间的重叠情况，而不是所有类别的边界框一起计算。在进行NMS处理时，通常只处理同一种类别的边界框，对于不同类别的边界框，需要分别进行处理。    
具体来说，如果在进行NMS处理时，需要处理多个类别的边界框，那么每个类别的边界框都需要进行查重处理。不同类别之间的边界框是互不干扰的，不会相互干扰或抵消。因此，在计算IOU值时，只有同一类别的边界框才会被考虑。最终，NMS算法会分别对每个类别输出过滤后的检测结果，从而得到针对每个类别的最终目标检测结果。    

置信度和IOU阈值     
NMS算法中，置信度（confidence）和IOU阈值（IOU threshold）是调节模型性能的两个关键超参数。    
在YOLO5中，默认的置信度阈值为0.25，即只有检测到的物体置信度高于0.25时，才会将其视为真正的检测结果。如果将此参数调低，则模型会更加敏感，但可能会导致一些误检和漏检。    
而IOU阈值则默认为0.45，即只有检测框之间的重叠程度大于0.45时，才会将其视为同一个目标的不同检测结果。这个参数能够控制模型的准确性和召回率。
当然，具体的置信度和IOU阈值也会根据具体的应用场景和任务需求进行调整。    
越小的置信度，表明将会有更多的框体被预测出来，所以模型将会更敏感，但是是建立在损失了置信度的情况上的，可能结果会乱打框    
越大的IOU，表明将会有更多的框体被保留，但是会出现框体重叠严重的情况，对于本身图像重叠严重的场景，比如地铁口拥挤的佩戴口罩的人群，如果检测目标是口罩配搭情况，则可以根据拥挤的场景，适当的调高IOU的值。   


![alt text](assets_picture/detect_track_keypoint/image-4.png)    

Yolo的CNN网络将输入的图片分割成SxS网格，然后每个单元格负责去检测那些中心点落在该格子内的目标，如图所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。    
![alt text](assets_picture/detect_track_keypoint/image-5.png)
在检测算法的实际使用中，一般都有这种经验: 对不同大小的bounding box预测中，相比于大box大小预测偏一点，小box大小测偏点肯定更不能被忍受。所以在Loss中同等对待大小不同的box是不合理的。为了解决这个问题，作者用了一个比较取巧的办法，即对W和h求平方根进行回归。从后续效果来看，这样做很有效，但是也没有完全解决问题。    
![alt text](assets_picture/detect_track_keypoint/image-6.png)

其中x,y就是预测边界框的中心坐标，中心坐标（x，y）的预测值 是相对于该单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，与单元格对齐（即相对于当前grid cell的偏移值），使得范围变成0到1，而边界框的w和h的预测值是相对于整个图片的宽和高的比例（即w和h进行归一化，分别除以图像的w和h，这样最后的w和h就在0到1范围了）     

另外每一个单元格（grid cell）都有C个类别的概率预测值，其表示的是由该单元格负责预测的边界框，在包含目标条件下属于各个类别的概率。但是这些概率值其实是在各个边界框置信度下的条件概率，即p(classi  | object).     
所谓置信度其实就是这个边界框含有目标的可能性大小与这个边界框的准确度的乘积。前者记为Pr(object),当边界框的为背景时（没有目标），Pr(object)=0,当边界框包含目标时，Pr(object)=1,后者记为边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表示，记为iou ，因此置信度为Pr(object)*iou       
前面已经有每一个单元格（grid cell）的C个类别的概率预测值Pr(class i  | object)，我们可以计算每个边界框的类别置信度：   
![alt text](assets_picture/detect_track_keypoint/1709898906994.png)

边界框类别置信度反映的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏     
每个边界框的类别置信度的过程图如下：这里把图片分割成了7*7的网格，每个单元格有2个预测边框，一共有20个类别，则整张图片共有7*7*2个边框，每个边框的类别置信度为20*1   
![alt text](assets_picture/detect_track_keypoint/image-7.png)    
即得到每个边界框属于20类的confidence score。也就是说最后会得到20*（7*7*2）=20*98的置信度矩阵。    
这个30是20+2*5，20代表类别数量，2代表每一个单元格有2个边界框，5代表（x,y,w,h,c）,具体含义前面讲过，7*7是单元格的数量。我们可以将其划分为三个部分：①类别概率部分，[7,7,20]，②边界框置信度部分，[7,7,2]，③边界框部分，[7,7,2,4]，类别概率部分*边界框置信度部分=边界框类别置信度（矩阵[7,7,2]乘以[7,7,20],为了方便计算，我们可以先将它们各补一个维度来完成[7,7,2,1]×[7,7,1,20]），两者相乘可以得到边界框类别置信度[7,7,2,20],这里总共有7*7*2=98个边界框     

接下来有两种策略得到边界框的类别结果和置信度？？？？有什么区别？   

第一种策略：对于每个预测框选择类别置信度最大的类别作为该预测框的类别标签，然后通过上一步得到了每个预测框的类别标签以及该类别的置信度，然后设置置信度阈值，将小于该置信度阈值的边框过滤掉，经过这样处理后，剩下的就是置信度比较高的边框，然后对这些预测框进行NMS算法处理，最后留下来的检测结果。   

这里提一下非极大值抑制算法（NMS），NMS算法主要解决一个目标被多次检测到的问题，比如人脸识别，如果人脸被多个边界框检测到，这时我们通过nms算法得到的是一个效果最好的检测框；NMS算法原理是首先从所有预测边界框中选择类别置信度最大的边界框，然后计算该边界框与剩余其他边界框进行IOU（交并比），如果其IOU值大于一定阈值（重复度过高），则将该边界框过滤掉，接下来对剩余的边界框重复上述过程，直至处理完所有的边界框。   

第二种策略：是原YOLO论文中使用的策略，首先对每个类别使用NMS，然后再确定各个边界框的类别，其过程如下图4所示，其过程是对于7*7*2=98个边界框，首先设置一定阈值，然后对每一个边界框的类别置信度与阈值作比较，如果小于该阈值，则将该类别置信度设置为0，接着对所有边界框的置信度从高到低做排序，然后对所有边界框分类别的（矩阵的每一行）进行NMS，得到一个最佳边界框获得该类别以及其置信度（该过程NMS:针对某一类别，选择类别置信度最大的bounding box，然后计算它和其它bounding box的IOU值，如果IOU大于阈值0.5，说明重复率较大，该置信度设为0，如果IOU不大于阈值0.5，则不改，再选择该行剩下的置信度里面最大的那个bounding box，然后计算该bounding box和其它bounding box的IOU，重复比较阈值过程，直到这一行所有的边界框结束为止；然后进行下一个类别，每一类别重复以上过程直到最后），这里不是剔除其他边界框，最后从每一个边界框中选择最大的类别置信度作为该边界框的类别标签以及置信度，最后筛选出置信度大于0的边界框作为检测的最后结果，如果小于0，说明这个边界框里面没有物体，跳过即可。    

#### yolo v3 对损失函数的讨论
宽高w、h的loss使用均方差损失函数。    
中心点的坐标x、y的loss、置信度c的loss和目标类别p的loss使用交叉熵损失函数。    
![alt text](assets_picture/detect_track_keypoint/image-12.png)    
![alt text](assets_picture/detect_track_keypoint/image-13.png)    
对于中心点的坐标x、y的 loss，DarkNet官方代码实现的YOLOV3里面坐标损失用的是交叉熵BCE Loss，而YOLOV3官方论文里面说的是均方差MSE Loss。    
对于宽高w、h的loss是均方差MSE Loss，因为没有经过sigmoid，而x、y是交叉熵BCE Loss因为经过了sigmoid。   
![alt text](assets_picture/detect_track_keypoint/image-14.png)
对于目标类别obj 的loss，Logistic回归正好方差损失和交叉熵损失的求导形式是一样的，都是output - label的形式。    
本这里应该用（二元分类的）交叉熵损失函数的，但在代码里也可直接用均方差损失函数代替。   
![alt text](assets_picture/detect_track_keypoint/image-15.png)    


#### 网络模型
网络上采用的是GoogLeNet，24个卷积层+2个全连接层，卷积层主要用来提取特征，全连接层主要用来预测类别概率和坐标。输入的是448*448，最后的输出是7*7*30，这个30是20+2*5，20代表类别数量，2代表每一个单元格有2个边界框，5代表（x,y,w,h,c）,具体含义前面讲过，7*7是单元格的数量。模型如下图3    
![alt text](assets_picture/detect_track_keypoint/image-8.png)
原文YOLO作者先在ImageNet数据集上预训练网络，而且网络只采用fig3的前面20个卷积层，输入是224*224大小的图像。然后在检测的时候再加上随机初始化的4个卷积层和2个全连接层，同时输入改为更高分辨率的448*448。  
③Relu层改为pRelu，即当x<0时，激活值是0.1*x，而不是传统的0。   

前面已经讲过YOLO的cnn模型（GoogleNet）,在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用之前图中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。整个网络的流程如下图所示：   
![alt text](assets_picture/detect_track_keypoint/image-10.png)    
![alt text](assets_picture/detect_track_keypoint/image-16.png)       


六.YOLO的优缺点    
优点：   
第一点Yolo采用一个CNN网络来实现检测，是单管道策略，其训练与预测都是end-to-end，所以Yolo算法比较简洁且速度快。    
第二点由于Yolo是对整张图片做卷积，所以其在检测目标有更大的视野，它不容易对背景误判。    

缺点：  
第一点Yolo各个单元格仅仅预测两个边界框，而且属于一个类别，如果一个单元格有两个以上的目标，就只能预测一个，对于小目标物体以及物体比较密集的也检测不好，比如一群小鸟     
第二点定位不准确，Yolo对于在物体的宽高比方面泛化率低，就是无法定位不寻常比例的物体。     
分类转换成了回归    
高召回率但低准确率：通常有更高的召回率（检测到更多的真实目标）但更低的准确率（更多的误检）     
因为YOLO中每个cell只预测两个bbox和一个类别，这就限制了能预测重叠或邻近物体的数量，比如说两个物体的中心点都落在这个cell中，但是这个cell只能预测一个类别    
YOLO是根据训练数据来预测bbox的，但是当测试数据中的物体出现了训练数据中的物体没有的长宽比时，YOLO的泛化能力低    


#### 流程问题
![alt text](assets_picture/detect_track_keypoint/image-17.png)     
用上图举例，设左下角格子假设坐标为 (1,1)，小狗所在的最小包围矩形框的中心，落在了 (2,3) 这个格子中。那么7✖7个格子中，(2,3) 这个格子负责预测小狗，而那些没有物体中心点落进来的格子，则不负责预测任何物体。    
当然，是7还是9，上图中的参数S，可以自己修改，精度和性能会随之有些变化    
![alt text](assets_picture/detect_track_keypoint/image-18.png)     
![alt text](assets_picture/detect_track_keypoint/image-19.png)     
视作回归问题的yolo v1     
![alt text](assets_picture/detect_track_keypoint/image-20.png)     
![alt text](assets_picture/detect_track_keypoint/image-21.png)   

正负样例    
正样本：   
对象中心落在网格内：对于每个真实对象（ground truth object），其边界框（bounding box）的中心落在哪个网格单元内，那个网格单元就负责预测这个对象。因此，该网格单元和与之对应的预测边界框成为“正样本”   
存在性置信度：与该网格相关联的“对象存在的置信度”应该接近1   
类别标签：该网格单元还需要预测该对象的类别   
负样本：   
对象中心不落在网格内：如果一个网格单元内没有任何真实对象（ground truth object）的中心，那么该网格单元就是一个“负样本”   
存在性置信度：与这些负样本网格相关联的“对象存在的置信度”接近0   

![alt text](assets_picture/detect_track_keypoint/image-22.png)    

#### 技巧
回归offset代替直接回归坐标   
(x,y)不直接回归中心点坐标数值，而是回归相对于格点左上角坐标的位移值。例如，第一个格点中物体坐标为 (2.3,3.6) ，另一个格点中的物体坐标为(5.4,6.3)，这四个数值让神经网络暴力回归，有一定难度。所以这里的offset是指，既然格点已知，那么物体中心点的坐标一定在格点正方形里，相对于格点左上角的位移值一定在区间[0, 1)中。让神经网络去预测 (0.3,0.6) 与 (0.4,0.3) 会更加容易，在使用时，加上格点左上角坐标(2,3)、(5,6)即可    

同一格点的不同预测框有不同作用    
前文中提到，每个格点预测两个或多个矩形框。此时假设每个格点预测两个矩形框。那么在训练时，见到一个真实物体，我们是希望两个框都去逼近这个物体的真实矩形框，还是只用一个去逼近？或许通常来想，让两个人一起去做同一件事，比一个人做一件事成功率要高，所以可能会让两个框都去逼近这个真实物体。但是作者没有这样做，在损失函数计算中，只对和真实物体最接近的框计算损失，其余框不进行修正。这样操作之后作者发现，一个格点的两个框在尺寸、长宽比、或者某些类别上逐渐有所分工，总体的召回率有所提升    

使用非极大抑制生成预测框     
在推理时，使用物体的类别预测最大值p乘以预测框的最大值c，作为输出预测物体的置信度。这样也可以过滤掉一些大部分重叠的矩形框。输出检测物体的置信度，同时考虑了矩形框与类别，满足阈值的输出更加可信    


### YOLO V2
![alt text](assets_picture/detect_track_keypoint/image-23.png)    
Batch Normalization（批归一化）    
在YOLOv1中，每一层卷积的结构都是线性卷积和激活函数，并没有使用批归一化（batch normalization，简称BN）。于是，在YOLOv2中，YOLO作者为YOLOv1添加了BN层，即卷积层的组成从原先的线性卷积与激活函数的组合改进为后来常用的“卷积三件套”：线性卷积、BN层以及激活函数    
![alt text](assets_picture/detect_track_keypoint/image-24.png)    
在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用dropout     
![alt text](assets_picture/detect_track_keypoint/image-25.png)    
BN和Dropout不能一起用    

High Resolution Classifier（分类网络高分辨率预训练）   
backbone网络在ImageNet上看得都是224×224的低分辨率图像，突然看到448×448的高分辨率图像，难免会“眼晕”。为了缓解这一问题，作者将已经在224×224的低分辨率图像上训练好的分类网络又在448×448的高分辨率图像上进行微调，共微调10个轮次。微调完毕后，再去掉最后的全局平均池化层和softmax层，作为最终的backbone网络    

Convolutional With Anchor Boxes（Anchor Box-替换全连接层）   
更加容易学习和收敛   
在v2中，神经网络不对预测矩形框的宽高的绝对值进行预测，而是预测与Anchor框的偏差（offset），每个格点指定n(5)个Anchor框。在训练时，最接近ground truth的框产生loss，其余框不产生loss。在引入Anchor Box操作后，mAP由69.5下降至69.2，原因在于，每个格点预测的物体变多之后，召回率大幅上升，准确率有所下降，总体mAP略有下降   

v2中移除了v1最后的两层全连接层，全连接层计算量大，耗时久。文中没有详细描述全连接层的替换方案，这里笔者猜测是利用1*1的卷积层代替（欢迎指正），具体的网络结构原文中没有提    
![alt text](assets_picture/detect_track_keypoint/image-26.png)     

Dimension Clusters（Anchor Box的宽高由聚类产生）
Anchor Box的宽高不经过人为获得，而是将训练数据集中的矩形框全部拿出来。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标： ![alt text](assets_picture/detect_track_keypoint/1709910971645.png) 聚类选择5个Anchor      

Direct location prediction（绝对位置预测）   
作者又对边界框的预测方法做了相应的调整   
首先，对每一个边界框，YOLO仍旧去学习中心点偏移量$t_x$和$t_y$。我们知道，这个中心点偏移量是介于01范围之间的数，在YOLOv1时，作者没有在意这一点，直接使用线性函数输出，这显然是有问题的，在训练初期，模型很有可能会输出数值极大的中心点偏移量，导致训练不稳定甚至发散。于是，作者使用sigmoid函数使得网络对偏移量的预测是处在01范围中。我们的YOLOv1+正是借鉴了这一点   
其次，对每一个边界框，由于有了边界框的尺寸先验信息，故网络不必再去学习整个边界框的宽高了。假设某个先验框的宽和高分别为$p_w$和$p_h$，网络输出宽高的偏移量为$t_w$和$t_h$，则使用公式即可解算出边界框的宽和高？？？？      
![alt text](assets_picture/detect_track_keypoint/image-27.png)    

Fine-Grained Features（细粒度特征）    
不难发现，特征图在经过reorg操作的处理后，特征图的宽高会减半，而通道则扩充至4倍，因此，从backbone拿出来的26×26×512特征图就变成了13×13×2048特征图。这种特殊降采样操作的好处就在于降低分辨率的同时，没丢掉任何细节信息，信息总量保持不变。引入更多的细节信息，确实有助于提升模型的检测性能    

Multi-Scale Training（多尺寸训练）    
在训练网络时，每训练迭代10次（常用iteration表示训练一次，即一次前向传播和一次反向传播，而训练一轮次则用epoch，即数据集的所有数据都经过了一次迭代），就从{320，352，384，416，448，480，512，576，608}选择一个新的图像尺寸用作后续10次训练的图像尺寸。注意，这些尺寸都是32的整数倍，因为网络的最大降采样倍数就是32。    

联合训练方法思路简单清晰   
当输入是检测数据集时，标注信息有类别、有位置，那么对整个loss函数计算loss，进行反向传播；当输入图片只包含分类信息时，loss函数只计算分类loss，其余部分loss为零。当然，一般的训练策略为，先在检测数据集上训练一定的epoch，待预测框的loss基本稳定后，再联合分类数据集、检测数据集进行交替训练，同时为了分类、检测数据量平衡，作者对coco数据集进行了上采样，使得coco数据总数和ImageNet大致相同。   
而分类检测数据集联合，可以扩充识别物体种类。例如，在检测物体数据集中，有类别人，当网络有了一定的找出人的位置的能力后，可以通过分类数据集，添加细分类别：男人、女人、小孩、成人、运动员等等。这里会遇到一个问题，类别之间并不一定是互斥关系，可能是包含（例如人与男人）、相交（运动员与男人），那么在网络中，该怎么对类别进行预测和训练呢？
作者使用WordTree，解决了ImageNet与coco之间的类别问题    
树结构表示物体之间的从属关系非常合适，第一个大类，物体，物体之下有动物、人工制品、自然物体等，动物中又有更具体的分类。此时，在类别中，不对所有的类别进行softmax操作，而对同一层级的类别进行softmax   

计算损失   
![alt text](assets_picture/detect_track_keypoint/image-28.png)    
第一项loss是计算background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background，需要计算noobj的置信度误差   

第二项是计算先验框与预测宽的坐标误差，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状    

第三大项计算与某个ground truth匹配的预测框各部分loss值，包括坐标误差、置信度误差以及分类误差。   
匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1）   
计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。    
在计算obj置信度时，在YOLOv1中target=1，？？？？而YOLOv2增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。     
YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框     

尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss    

在计算boxes的和误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：l.coord_scale * (2 - truth.w*truth.h)，这样对于尺度较小的boxes其权重系数会更大一些，起到和YOLOv1计算平方根相似的效果    

正负样例   
正样本（Positive Samples）：   
IoU（交并比）：在训练过程中，YOLOv2通常使用IoU（Intersection over Union）来确定哪些锚框（Anchor Boxes）与真实边界框（Ground Truth Bounding Boxes）匹配。一般情况下，如果某个锚框与真实边界框的IoU超过一个预定的阈值（通常为0.5或更高），则该锚框被认为是一个正样本
最佳锚框：对于每个真实边界框，选择与其具有最高IoU的锚框，即使IoU值低于阈值   
分类标签：正样本的分类标签通常与其对应的真实边界框的类别标签相同   
负样本（Negative Samples）：   
低IoU：通常，具有低IoU（与所有真实边界框的IoU都低于某一阈值）的锚框被认为是负样本   
背景分类：负样本通常被分配一个“背景”分类标签，这意味着这些锚框不包含任何感兴趣的目标   

通过这种方式，YOLOv2试图在正样本中捕获各种形状和尺寸的目标，同时也生成负样本以帮助模型更好地区分目标和背景。   

优缺点    
Darknet-19：YOLOv2提出了一个全新的19层网络架构（Darknet-19），用于特征提取。该网络既轻量级也高效   
锚框（Anchor Boxes）：使用锚框可以更准确地检测不同形状和大小的物体，改进了对高度重叠物体的检测能力   
多尺度检测：通过引入多尺度检测，YOLOv2能更好地检测不同大小的物体。这极大地提高了对小目标的检测能力   
速度快：YOLOv2是为实时性能优化的。只需要一次前向传播，就能完成目标的检测   
联合训练（Joint Training）：YOLOv2可以同时在检测和分类任务上进行训练，这提高了其泛化性能   
位置敏感性和类别预测：YOLOv2对于物体位置的敏感性减小，而对于类别预测的准确性提高   
对小目标的检测不佳：尽管进行了多尺度检测，YOLOv2对小目标的检测性能仍然不如一些其他算法   
处理高度重叠目标的挑战：尽管引入了锚框，但在高度重叠或密集的物体情况下，检测性能仍然有待提高   
位置精度较低：虽然YOLOv2在类别识别方面表现出色，但在定位物体方面的精度相对较低   


### yolo v3 
![alt text](assets_picture/detect_track_keypoint/image-29.png)   
![alt text](assets_picture/detect_track_keypoint/image-30.png)    
??????    
为什么是小尺寸特征图用于检测大尺寸物体     
![alt text](assets_picture/detect_track_keypoint/image-31.png)   
![alt text](assets_picture/detect_track_keypoint/image-32.png)    
![alt text](assets_picture/detect_track_keypoint/image-33.png)   
![alt text](assets_picture/detect_track_keypoint/image-34.png)   
在Yolov1中，网络直接回归检测框的宽、高，这样效果有限。所以在Yolov2中，改为了回归基于先验框的变化值，这样网络的学习难度降低，整体精度提升不小。    
Yolov3沿用了Yolov2中关于先验框的技巧，并且使用k-means对数据集中的标签框进行聚类，得到类别中心点的9个框，作为先验框。在COCO数据集中（原始图片全部resize为416 × 416），九个框分别是 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h    

置信度在输出85维中占固定一位，由sigmoid函数解码即可，解码之后数值区间在[0，1]中   
COCO数据集有80个类别，所以类别数在85维输出中占了80维，每一维独立代表一个类别的置信度。使用sigmoid激活函数替代了Yolov2中的softmax，取消了类别之间的互斥，可以使网络更加灵活     
三个特征图一共可以解码出 8 × 8 × 3 + 16 × 16 × 3 + 32 × 32 × 3 = 4032 个box以及相应的类别、置信度。这4032个box，在训练和推理时，使用方法不一样：    
训练时4032个box全部送入打标签函数，进行后一步的标签以及损失函数的计算    
推理时，选取一个置信度阈值，过滤掉低阈值box，再经过nms（非极大值抑制），就可以输出整个网络的预测结果了    

计算损失    
![alt text](assets_picture/detect_track_keypoint/image-35.png)     
x、y、w、h使用MSE作为损失函数，也可以使用smooth L1 loss（出自Faster R-CNN）作为损失函数。smooth L1可以使训练更加平滑。置信度、类别标签由于是0，1二分类，所以使用交叉熵作为损失函数     

正负样本产生
![alt text](assets_picture/detect_track_keypoint/image-36.png)    


![alt text](assets_picture/detect_track_keypoint/image-37.png)   
![alt text](assets_picture/detect_track_keypoint/image-38.png)   





## 检测算法
### PP-YOLO检测流程
（1）首先，将待检测的图像进行预处理，包括图像大小的缩放、像素值的归一化等操作，以使其能够适应PP-YOLO模型输入的要求。这里输入图片的要求与YOLOv3所输入的图片要求一致，即网络输入的图片大小必须为32的整数倍。   
（2）使用特征提取网络ResNet50-vd，对经过预处理的图像进行多层特征提取，得到图像的一系列特征图。然后，利用特征金字塔算法将不同尺度的特征图融合，以便于检测不同大小的物体。   
（3）针对不同特征尺度，使用聚类以及K-Means算法，基于训练数据集的目标边界框宽高比例将输入的网络图像划分成一系列网格，生成一组默认锚框（anchor），由此产生候选区域。?????    
（4）对于每一个特征图上的每个锚框，通过卷积神经网络预测该锚框中包含的物体的类别概率和边界框回归信息。?????   
（5）利用非极大值抑制（NMS）算法，过滤重复的检测结果，仅保留检测质量最高的结果。同时，根据不同目标的置信度范围，对检测框进行颜色编码，以实现更好的可视化效果。???     
（6）将最终的检测结果（类别、位置、置信度）输出，并可通过图像绘制等方式呈现给用户。   
![alt text](assets_picture/yolo/1.png)








### PP-YOLOE  
PP-YOLOE-l 在COCO数据集上达到了51.4mAP。相比较PP-YOLOv2提升1.9AP和13.35%的速度，相比较YOLOX提升1.3AP和24.96%的速度。   
而PP-YOLOE中主要的改进点是：anchor-free，powerful backbone and neck，TAL动态label assign，ET-head。   
![Alt text](assets_picture/yolo/image.png)   
2.1 锚框机制    
Anchor-free（Anchor-base模型引入超参数，依赖手工设计，对不同的数据集需要单独聚类），在每个像素上放置一个锚点，为三个检测头设置GT尺寸的上届和下界。计算GT的中心，选择最近的锚点做正样本。Anchor-free方式使mAP比Anchor-base下降0.3，但是速度有所提升，具体如下表所示。  
2.2 标签分配   
使用TOOD中的TAL(Task Aligned Learing),显性的对齐分类最优点和位置回归最优点。注TOOD论文中，提出任务对齐头部（T-Head）和任务对齐学习（TAL）。T-head在学习任务交互特征和任务特定特征之间提供了更好的平衡，并通过任务对齐预测器学习对齐的灵活性提高,TAL通过设计的样本分配方案和任务对齐损失，明确地拉近（甚至统一）两个任务的最优锚点 TAL包含任务对齐指标、正样本选择标准，并将任务对齐指标与原先的的分类损失、回归损失进行联立[其本质就是根据任务对齐指标调节不同样本在反向传播时loss的权重,具体可以参考https://hpg123.blog.csdn.net/article/details/128725465]。PP-YOLOE其实也尝试过多种标签对齐方式，具体如下所示，可见TAL效果是最佳的。   
![Alt text](assets_picture/yolo/image-1.png)       
2.3 loss设计   
对于分类和定位任务，分别选择了varifocal loss（VFL）和distribution focal loss（DFL）。PP-Picodet成功将VFL和DFL语义到目标检测中。VFL与quality focal(QFL)不同，VFL使用目标评分来衡量正样本loss的权重（可提升正样本loss的贡献，使模型更多关注高质量正样本，解决了NMS过程中classification score 和 IoU/centerness score 训练测试不一致【训练时两个孤立，nms时两个联立】），两者都使用带有IOU感知的分类评分作为预测目标。整体loss设计如下所示，其中$ \hat{t} $ 表示标准化的目标分数，使用ET-head提升了0.5的map   
![Alt text](assets_picture/yolo/image-2.png)     
DFL（distribution focal loss）：为了解决bbox不灵活的问题，提出使用distribution[将迪克拉分布转化为一般分布]预测bbox[预测top、left、right、bottom]。     
![Alt text](assets_picture/yolo/image-3.png)   




PP-YOLOE 还避免使用诸如可变形卷积或者 Matrix NMS 之类的特殊算子，使 PP-YOLOE 全系列模型能轻松地部署在 NVIDIA V100 和 T4 这样的云端 GPU 架构、Jetson 系列的移动端 GPU 和高性能的 FPGA 开发板上。   
具体的结构包括以下三大部分：   
■ 可扩展的 backbone 和 neck   
我们设计了 CSPRepResNet 作为 backbone，neck 部分也采用了新设计的 CSPPAN 结构，backbone 和 neck 均以我们提出的 CSPRepResStage 为基础模块。新的 backbone 和 neck 在增强模型表征能力的同时提升了模型的推理速度，并且可以通过 width multiplier 和 depth multiplier 灵活地配置模型的大小。   
■ TAL（Task Alignment Learning）   
为了进一步提升模型的精度，我们选用了 TOOD [1]中的动态匹配算法策略 TAL。TAL 同时考虑分类和回归，使得匹配结果同时获得了最优的分类和定位精度。   
■ Efficient Task-aligned head   
检测 head 方面，我们在 TOOD 的 T-head 基础上进行改进。  
首先，使用 ESE block 替换掉了原文中比较耗时的 layer attention，使得模型在保证精度不变的同时提升了速度。   
其次，由于 T-head 使用了可变形卷积算子，对硬件部署不友好，我们在分类分支使用 shortcut 替换 cls-align 模块，回归分支使用积分层替换掉含有可变形卷积的 reg-align 模块，以上两个改进使得 head 变得更为高效、简洁且易部署。   
最终，我们进一步使用 VFL（VariFocal Loss）替换 BCE 作为分类分支 Loss，达到了速度精度双高的目的。   




### PPYOLOE+   
PPYOLOE+表示在object365中进行了预训练（其模型结构配置文件与PPYOLOE一模一样，只是在backbone中block分支中增加alpha参数）的PPYOLOE模型。两个模型在ATSSAssigner与TaskAlignedAssigner的epoch数上存在不同，ppyoloe的static_assigner_epoch为100，ppyoloe+的为30【经过预训练后ppyoloe+对ATSSAssigner的依赖降低】.


### yolov5_based_on_transformer







# 跟踪算法
### BoT-SORT





# 关键点算法
### HRNet
首先采用高分辨率的子网络作为第一阶段；然后逐渐添加高分辨率到低分辨率的子网络，得到多个阶段的输出；最后，并行的连接多分辨率子网络的输出. 其进行了多次多尺度融合，因此，使得每一个高分辨率到低分辨率的特征图表示，都可以一次又一次地从其它并行表示分支接收信息，从而得到信息更丰富的高分辨率表示. 最终，网络输出的关键点heatmap 更精确，空间分辨率精度更高；    